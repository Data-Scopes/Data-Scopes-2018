# Data Scopes

- Coherent methods for using digital data in humanities research
- Data scope: you want to analyse a certain aspect of your materials,
    - but the "raw" data is not suitable for direct analysis.
- You have to do something with the data. Questions:
    - What do I have to do to make data suitable?
    - How do I do that?
    - What should I document of this process to I can share it with others?
    - Which parts of this process are specific to my analysis and which are generically applicable?

## Focus on data interactions:
- It's not about specific tools
    - Many tutorials online for that, e.g. the excellent [Programming Historian](https://programminghistorian.org)
- It's about what you're doing, how and why
- Translating research questions, assumption and interpretations to data interactions
- Consequences of interactions for questions, assumptions and interpretations

## Frameworks:

- Data Scope: select, model, normalise, link, classify
- Other models focusing on process:
    - [Scholarly Primitives](https://www.google.com/sheets/about/) (John Unsworth): discover, annotate, compare, refer, sample, illustrate, represent
    - [Data Visualization](https://www.google.com/sheets/about/) (Ben Fry): acquire, parse, filter, mine, represent, refine, interact

## Data Scope Schema

<img src="../../images/schema-data-scope.jpg"/>

## Modelling:

- "heuristic process of constructing and manipulating models" (McCarty, 2004)
- model: 
    - "a representation of something for purposes of study,"
    - "or a design for realizing something new"
- model determines what aspects of data to focus on
    - structures data in sources around research focus
    - transforms data, affects interpretation!

## Modelling:

- defining data axes: 
    - persons, organisations, locations, dates, topics, 
    - themes, life courses, events, actions, decisions
- defining categories or classes along those axes: 
    - roles of people and organisations
    - periods, regions
    - Research stages:
- model is updated as research progresses
    - this updating reflects growing insights

## Selecting:

- Which materials do I include? Which data elements do I focus on?
    - data axes
- algorithmic selection:
    - everything matching a (set of) keyword(s)
    - documents by type, creator, title, size, ...
- What are consequences of these selections?
    - What am I excluding?

## Normalizing:

- map variation, 
    - essential for next step: linking 

## Linking:

- linking across different corpora
    - e.g. mentions of same person, location, date, ...

## Classifying:

- reducing complexity
    - especially for low-frequency items



